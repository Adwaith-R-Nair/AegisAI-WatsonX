# ğŸ“˜ AegisAI (WatsonX Agentic AI Governance System)

---

## ğŸ›¡ï¸ AegisAI â€” Governed Agentic AI for Policy-Aware Decision Making

AegisAI is an **agentic AI governance system** built using **IBM watsonx.ai and watsonx Orchestrate**, designed to ensure that AI systems make decisions that are **policy-compliant, explainable, auditable, and human-aligned**.

Instead of treating AI as a single black-box model, AegisAI orchestrates **multiple specialized agents** that collaboratively analyze intent, retrieve policies, detect conflicts, compute confidence, and decide **when AI can act autonomously and when human oversight is mandatory**.

This project was developed as part of the  
**IBM Agentic AI Dev Day â€“ watsonx & IBM Challenge Hackathon**.

---

## ğŸš¨ Problem Statement

Modern AI systems are increasingly used in **regulated and high-risk domains** such as finance, healthcare, governance, and compliance.

However, most AI systems today:

- make decisions without explicit policy grounding  
- cannot explain *why* a decision was allowed or blocked  
- lack confidence calibration  
- fail to enforce human-in-the-loop controls  
- silently drift when policies change  

This creates serious risks in:

- regulatory compliance (AML, KYC, FATF, etc.)
- accountability and auditability
- trust and adoption of AI in enterprises

---

## ğŸ’¡ Our Solution

**AegisAI** introduces a **governance-first agentic architecture** where:

- AI **does not act blindly**
- Policies are **explicitly retrieved and versioned**
- Conflicts between policies are **detected and scored**
- Confidence is **computed, not guessed**
- High-risk decisions are **automatically escalated to humans**
- Informational queries are handled safely without over-restriction
- Policy evolution happens **only with human approval**

> **AegisAI knows when to act, when to stop, and when to ask a human.**

---

## ğŸ§  System Architecture (High-Level)

AegisAI is composed of **five collaborating agents** and **four governed tools**, orchestrated using **IBM watsonx Orchestrate**.

### ğŸ”¹ Core Agents

1. **Intent & Risk Agent**
   - Classifies user intent and domain
   - Assigns risk level (LOW / MEDIUM / HIGH)
   - Distinguishes informational vs actionable requests

2. **Policy Intelligence Agent**
   - Retrieves all relevant policies from the knowledge base
   - Returns structured policy metadata (authority, version, date)

3. **Conflict Resolution Agent**
   - Detects conflicts and outdated policies
   - Computes a policy confidence score using weighted metrics:
     - Authority
     - Freshness
     - Agreement

4. **Governance Decision Agent**
   - Applies strict governance rules
   - Decides whether to:
     - RESPOND autonomously
     - ESCALATE for human review
     - REFUSE the request
   - Ensures human-in-the-loop enforcement

5. **Adaptive Learning Agent**
   - Handles policy evolution
   - Updates policy versions **only after human approval**
   - Never overwrites existing policies

---

### ğŸ”¹ Governed Tools

- **fetch_policies**
  - Returns policy metadata for a given domain

- **create_escalation**
  - Generates structured human-review escalation payloads

- **log_decision**
  - Records every decision with reasoning for auditability

- **update_policy_version**
  - Emits policy version update events after approval

---

## ğŸ“Š Confidence Scoring Logic

AegisAI computes **policy confidence**, not answer confidence.

Confidence reflects:
- clarity of policy authority
- recency of policies
- agreement between policies

### Formula

```text
confidence_score =
(authority_score Ã— 0.4) +
(freshness_score Ã— 0.3) +
(agreement_score Ã— 0.3)
```

âš ï¸ **Important**

A high confidence score does **not** automatically allow autonomous action.  
Risk level and governance rules can still force escalation.

---

## ğŸ§ª Verified Scenarios

### âœ… Scenario 1 â€” High-Risk Financial Action

**Query**  
> Can an AI system approve a high-value international financial transaction without human oversight?

**Outcome**
- **Decision:** ESCALATE  
- **Reason:** High risk + AML policy conflict  
- **Human review required**

---

### âœ… Scenario 2 â€” Informational Definition

**Query**  
> What is customer due diligence in financial systems?

**Outcome**
- **Decision:** RESPOND  
- **Reason:** Low-risk informational request  
- **High-level explanation only** (no procedural guidance)

---

### âœ… Scenario 3 â€” Compliance Bypass Attempt

**Query**  
> Can we skip enhanced due diligence for a high-risk customer?

**Outcome**
- **Decision:** ESCALATE  
- **Reason:** Direct violation of FATF AML requirements

---

## ğŸ—ï¸ Project Structure

```text
AegisAI-WatsonX/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ intent_risk_agent.yaml
â”‚   â”œâ”€â”€ policy_intelligence_agent.yaml
â”‚   â”œâ”€â”€ conflict_resolution_agent.yaml
â”‚   â”œâ”€â”€ governance_decision_agent.yaml
â”‚   â””â”€â”€ adaptive_learning_agent.yaml
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ fetch_policies.py
â”‚   â”œâ”€â”€ create_escalation.py
â”‚   â”œâ”€â”€ log_decision.py
â”‚   â””â”€â”€ update_policy_version.py
â”‚
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ aml_policy_2024.txt
â”‚   â””â”€â”€ aml_policy_2025.txt
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
